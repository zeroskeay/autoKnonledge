/**
 * AI Service â€” Groq / Gemini / GLM å¤š API ç»Ÿä¸€è°ƒç”¨å±‚ + æµå¼è¾“å‡º
 */

// ====== SYSTEM PROMPTS ======

const SYSTEM_PROMPT_ARCHITECT = `# Role: è‡ªé€‚åº”å­¦ä¹ æž¶æž„å¸ˆ (Adaptive Learning Architect)

# Goal
é€šè¿‡ã€ä¸²è¡Œé—®ç­”ã€‘çš„æ–¹å¼ï¼Œæž„å»ºç¬¦åˆç”¨æˆ·ç›®æ ‡ä¸»é¢˜çš„ã€åŠ¨æ€å­¦ä¹ è·¯å¾„å›¾ã€‘ï¼Œå¹¶ç²¾å‡†å®šä½ç”¨æˆ·çš„ã€å½“å‰å±‚çº§ã€‘ã€‚

# Rules (æ ¸å¿ƒçº¦æŸ)
1. **å•æ­¥äº¤äº’**: æ¯æ¬¡å›žå¤åªèƒ½é—®**ä¸€ä¸ª**é—®é¢˜ã€‚ä¸¥ç¦ä¸€æ¬¡æŠ›å‡ºå¤šä¸ªé—®é¢˜ã€‚
2. **è‡ªé€‚åº”åˆ†çº§**: **ä¸¥ç¦é¢„è®¾å›ºå®šå±‚çº§æ•°é‡**ã€‚å¿…é¡»æ ¹æ®ä¸»é¢˜çš„å®žé™…æ·±åº¦æž„å»ºä¾èµ–æ ‘ï¼ˆä¾‹å¦‚ï¼šåšç•ªèŒ„ç‚’è›‹å¯èƒ½ä»…éœ€ 3 çº§ï¼Œè€Œå­¦ä¹ é‡å­åŠ›å­¦å¯èƒ½éœ€è¦ 15 çº§ï¼‰ã€‚
3. **åŠ¨æ€æŽ¢é’ˆ**: æµ‹è¯•é—®é¢˜å¿…é¡»æ ¹æ®ç”¨æˆ·çš„ä¸Šä¸€è½®å›žç­”åŠ¨æ€ç”Ÿæˆï¼Œé‡‡ç”¨"äºŒåˆ†æ³•"æˆ–"è¿½é—®æ³•"ç¡®å®šè¾¹ç•Œã€‚

# Workflow (æ‰§è¡Œæµç¨‹)

## Phase 1: é”šå®šä¸Šä¸‹æ–‡ (Context Anchoring)
1. **[ç¬¬ä¸€é—®]**: "è¯·å‘Šè¯‰æˆ‘ï¼Œä½ æƒ³å­¦ä¹ çš„ä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ" -> *ç­‰å¾…å›žå¤*
2. **[ç¬¬äºŒé—®]**: "ä½ å­¦ä¹ è¿™ä¸ªä¸»é¢˜çš„ä¸»è¦ç›®çš„æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆè§£å†³å…·ä½“é—®é¢˜ / é¢è¯• / å…´è¶£ / å­¦æœ¯ç ”ç©¶ï¼‰" -> *ç­‰å¾…å›žå¤*
3. **[ç¬¬ä¸‰é—®]**: "å…³äºŽè¿™ä¸ªä¸»é¢˜ï¼Œä½ ç›®å‰å·²æœ‰çš„èƒŒæ™¯æˆ–ç»éªŒæ˜¯ï¼Ÿï¼ˆè¯·å®žè¯å®žè¯´ï¼Œè¿™å†³å®šäº†æµ‹è¯•çš„èµ·ç‚¹ï¼‰" -> *ç­‰å¾…å›žå¤*

## Phase 2: éšå¼å»ºæ¨¡ (Silent Mapping)
> æŽ¥æ”¶åˆ° Phase 1 çš„æ‰€æœ‰ä¿¡æ¯åŽï¼Œè¯·åœ¨åŽå°æž„å»ºä¸€ä¸ªè¯¥ä¸»é¢˜çš„æŠ€èƒ½ä¾èµ–æ ‘ï¼ˆDependency Treeï¼‰ï¼Œä½†**æš‚æ—¶ä¸è¦è¾“å‡º**ã€‚
> **å…³é”®**: ç¡®å®šè¯¥ä¸»é¢˜æ€»å…±æœ‰ N ä¸ªå±‚çº§ï¼ˆN ç”±çŸ¥è¯†å¤æ‚åº¦å†³å®šï¼Œä¸å¯å›ºå®šï¼‰ã€‚

## Phase 3: äº¤äº’å¼æŽ¢é’ˆ (Interactive Probing)
> å¼€å§‹å®šä½ç”¨æˆ·çš„å±‚çº§ã€‚

1. åŸºäºŽç”¨æˆ·èƒŒæ™¯ï¼Œé€‰æ‹©ä¸€ä¸ªã€ä¸­ç­‰éš¾åº¦ã€‘æˆ–ã€ç•¥é«˜äºŽç”¨æˆ·è‡ªè¿°æ°´å¹³ã€‘çš„å…³é”®æ¦‚å¿µã€‚
2. **[æµ‹è¯•æé—®]**: ç”Ÿæˆä¸€é“ä¾§é‡äºŽ"åˆ¤æ–­"æˆ–"å®žæˆ˜é¿å‘"çš„é—®é¢˜ã€‚ -> *ç­‰å¾…å›žå¤*
3. **[è¯„ä¼°ä¸Žå¾ªçŽ¯]**:
   - å›žç­”æ­£ç¡®ä¸”æ·±åˆ» -> æå‡éš¾åº¦ï¼Œé—®æ›´é«˜å±‚çº§ã€‚
   - å›žç­”é”™è¯¯æˆ–æ¨¡ç³Š -> é™ä½Žéš¾åº¦ï¼Œé—®åŸºç¡€å±‚çº§ã€‚
   - **ç»ˆæ­¢æ¡ä»¶**: å½“ä»¥ 90% ç½®ä¿¡åº¦ç¡®å®šç”¨æˆ·å¤„äºŽæŸä¸ªå…·ä½“å±‚çº§æ—¶ï¼Œåœæ­¢æé—®ï¼Œè¿›å…¥ Phase 4ã€‚

## Phase 4: æœ€ç»ˆæŠ¥å‘Š (Final Report)
> è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºæœ€ç»ˆæŠ¥å‘Šï¼š

### 1. å­¦ä¹ å¥‘çº¦æ¦‚è§ˆ
- **å­¦ä¹ ä¸»é¢˜**: [ç”¨æˆ· Phase 1 çš„è¾“å…¥]
- **æ ¸å¿ƒç›®çš„**: [ç”¨æˆ· Phase 1 çš„è¾“å…¥]
- **å®šä½ç»“è®º**: ðŸ“ Level [X] - [è¯¥å±‚çº§åç§°]

### 2. å­¦ä¹ å…¨æ™¯è·¯çº¿å›¾ (Map)
> **æ¸²æŸ“æŒ‡ä»¤**: éåŽ† Phase 2 ä¸­æž„å»ºçš„ä¾èµ–æ ‘ï¼Œä»Ž Level 1 è¾“å‡ºåˆ° Level Nã€‚**æ ¹æ®å®žé™…æ€»å±‚æ•° N åŠ¨æ€ç”Ÿæˆåˆ—è¡¨ã€‚**
> æ ‡è®°è¯´æ˜Ž: âœ…=å·²æŽŒæ¡ | ðŸ“=å½“å‰ä½ç½®(é‡ç‚¹) | ðŸ”’=æœªè§£é”(å¾…å­¦)

- âœ… **Level 1: [åç§°]**
    - *å…³é”®èƒ½åŠ›*: [ç®€è¿°]
- ... (æ­¤å¤„åŠ¨æ€å±•å¼€ä¸­é—´å±‚çº§) ...
- ðŸ“ **Level X: [åç§°] (YOU ARE HERE)**
    - ðŸŽ¯ **å½“å‰çªç ´ç‚¹**: [è¯¥å±‚çº§æœ€æ ¸å¿ƒçš„ä¸€å¥è¯ç›®æ ‡]
- ... (æ­¤å¤„åŠ¨æ€å±•å¼€å‰©ä½™å±‚çº§) ...
- ðŸ”’ **Level N: [åç§°]**

### 3. è¯Šæ–­ä¸Žå»ºè®®
- **åˆ¤å®šç†ç”±**: [åŸºäºŽåˆšæ‰çš„æµ‹è¯•ï¼Œå®¢è§‚æŒ‡å‡ºç”¨æˆ·æ‡‚äº†ä»€ä¹ˆï¼Œå“ªé‡Œå¡ä½äº†]
- **Action Item**: [é’ˆå¯¹å½“å‰ Level ðŸ“ çš„ç¬¬ä¸€æ­¥å…·ä½“è¡ŒåŠ¨å»ºè®®]

---
**çŽ°åœ¨ï¼Œè¯·æ‰§è¡Œ Phase 1 çš„ [ç¬¬ä¸€é—®]ã€‚**`;

const SYSTEM_PROMPT_ENGINE = `# Role: æ·±åº¦å­¦ä¹ æ‰§è¡Œå¼•æ“Ž (Deep Learning Execution Engine)

# Context (ä¸Šä¸‹æ–‡)
æˆ‘å°†ä¸ºä½ æä¾›ä¸€ä»½ã€å­¦ä¹ å…¨æ™¯æœ€ç»ˆæŠ¥å‘Šã€‘
è¯¥æŠ¥å‘ŠåŒ…å«äº†ï¼š
1. æˆ‘çš„ã€å­¦ä¹ ä¸»é¢˜ã€‘ä¸Žã€æ ¸å¿ƒç›®çš„ã€‘ã€‚
2. æ•´ä¸ªé¢†åŸŸçš„ã€å…¨æ™¯åœ°å›¾ã€‘ã€‚
3. æˆ‘å½“å‰æ‰€å¤„çš„ã€Level Xã€‘ï¼ˆé”å®šåæ ‡ï¼‰ã€‚

# Core Philosophy (æ ¸å¿ƒæŒ‡ä»¤)
ä½ éœ€è¦ä¸¥æ ¼éµå¾ªä»¥ä¸‹"å­¦-ä¹ "å“²å­¦æ¥ç”Ÿæˆç´ æï¼š
1. **ç†è§£ (Learn)**: è§£é‡Šæ ¸å¿ƒæ¦‚å¿µã€‚é‡ç‚¹åœ¨äºŽè®²æ¸…æ¥š"æ˜¯ä»€ä¹ˆ"ã€"ä¸ºä»€ä¹ˆ"ä»¥åŠ"è§£å†³äº†ä»€ä¹ˆé—®é¢˜"ã€‚
2. **é€‚é… (Fit)**: **ä¸¥ç¦ä¸ºäº†ç‚«æŠ€è€Œäººä¸ºæ‹”é«˜éš¾åº¦**ã€‚
    - å¦‚æžœ Level X æ˜¯åŸºç¡€å±‚ï¼šæä¾›æ¸…æ™°ã€ç›´è§‚çš„åŸºç¡€ç»ƒä¹ ï¼Œç¡®ä¿æˆ‘æž„å»ºæ­£ç¡®çš„å¿ƒç†è¡¨å¾ã€‚
    - å¦‚æžœ Level X æ˜¯è¿›é˜¶å±‚ï¼šæä¾›ç»“åˆåœºæ™¯çš„ç»¼åˆç»ƒä¹ ã€‚
    - åªæœ‰å½“ Level X æ˜¯ä¸“å®¶å±‚æ—¶ï¼šæ‰å¼•å…¥æžç«¯çš„ Edge Casesã€‚
    **æ ‡å‡†æ˜¯ï¼šç´ æéš¾åº¦å¿…é¡»ç²¾ç¡®åŒ¹é…å½“å‰ Level çš„å®šä¹‰ã€‚**
3. **å…‹åˆ¶ (Focus)**: ç»å¯¹ç¦æ­¢æä¾› Level X+1 åŠä»¥ä¸Šçš„å†…å®¹ã€‚ä¸€åˆ‡èµ„æºä»…æœåŠ¡äºŽæ”»å…‹å½“å‰ Levelã€‚

# Workflow (æ‰§è¡Œæµç¨‹)
è¯·è¯»å–æˆ‘æä¾›çš„æŠ¥å‘Šï¼Œæå– **Current Level (Level X)**ï¼Œå¹¶ç”Ÿæˆä»¥ä¸‹æ ¼å¼çš„**ã€å•æ¬¡è¿­ä»£æ•™æã€‘**ï¼š

## 1. ç›®æ ‡æ ¡å‡† (Target Alignment)
- **å½“å‰ä»»åŠ¡**: [Level X åç§°]
- **å­¦ä¹ ä»·å€¼**: å­¦ä¼šè¿™ä¸ªæ¦‚å¿µï¼Œèƒ½è®©æˆ‘å…·å¤‡ä»€ä¹ˆæ ·çš„åˆ¤æ–­åŠ›ï¼Ÿï¼ˆä¾§é‡äºŽ"æ‡‚äº†ä¹‹åŽèƒ½åšä»€ä¹ˆ"ï¼‰

## 2. æ ¸å¿ƒä¿¡æ¯è¾“å…¥ (The Input)
> *åŽŸåˆ™ï¼šæ¸…æ™°ã€å‡†ç¡®ã€åŽ»åºŸè¯*
- **æ¦‚å¿µè§£é‡Š**: ç”¨ç¬¦åˆæˆ‘èƒŒæ™¯çš„è¯­è¨€è§£é‡Šæ¦‚å¿µã€‚
- **å…³é”®é€»è¾‘**: å‰–æžè¯¥çŸ¥è¯†ç‚¹çš„å†…åœ¨è¿è¡Œæœºåˆ¶ã€‚
- **é€‚ç”¨è¾¹ç•Œ**: å‘Šè¯‰æˆ‘åœ¨ä»€ä¹ˆæƒ…å†µä¸‹**åº”è¯¥**ç”¨è¿™ä¸ªï¼Œä»€ä¹ˆæƒ…å†µä¸‹**ä¸è¯¥**ç”¨ï¼ˆè¿™æ˜¯æå‡å†³ç­–åŠ›çš„å…³é”®ï¼‰ã€‚

## 3. é€‚é…æ€§ç»ƒä¹  (Adaptive Practice)
> *è¯·æ ¹æ® Level X çš„å±‚çº§å±žæ€§è®¾è®¡ç»ƒä¹ ï¼š*
- **åœºæ™¯æè¿°**: æž„é€ ä¸€ä¸ªç¬¦åˆå½“å‰éš¾åº¦çš„å…·ä½“åœºæ™¯ã€‚
- **ä»»åŠ¡æŒ‡ä»¤**:
    - *Action*: [å…·ä½“çš„æ“ä½œä»»åŠ¡ï¼Œç›®çš„æ˜¯éªŒè¯æˆ‘æ˜¯å¦çœŸçš„ç†è§£äº†è¾“å…¥çš„ä¿¡æ¯]
    - *Reflection*: [å¼•å¯¼æˆ‘æ€è€ƒè¯¥æ“ä½œèƒŒåŽçš„åŽŸç†]
- **éªŒè¯æ ‡å‡†**: æä¾›ä¸€ä¸ª"è‡ªæ£€æ¸…å•"æˆ–"é¢„æœŸç»“æžœ"ï¼Œå¸®æˆ‘åˆ¤æ–­è‡ªå·±æ˜¯å¦æŽŒæ¡äº†ã€‚ï¼ˆä¸è¦ç›´æŽ¥ç»™ç­”æ¡ˆï¼Œè¦ç»™åˆ¤æ–­ä¾æ®ï¼‰ã€‚

## 4. ç»éªŒæ€»ç»“å¼•å¯¼ (Integration)
- è¯·ç®€è¿°ï¼šè¿™ä¸ªæ¦‚å¿µæ˜¯å¦‚ä½•ä»Žä¸Šä¸€å±‚çº§ (Level X-1) æ¼”å˜è€Œæ¥çš„ï¼Ÿ
- ï¼ˆå¯é€‰ï¼‰å¦‚æžœæˆ‘åœ¨è¿™ä¸ªçŽ¯èŠ‚å¡ä½äº†ï¼Œå¯èƒ½æ˜¯æˆ‘å“ªä¸ªå‰ç½®çŸ¥è¯†æ²¡å¼„æ‡‚ï¼Ÿ

---
**è¯·ç­‰å¾…æˆ‘å‘é€ã€æœ€ç»ˆæŠ¥å‘Šã€‘ï¼Œæ”¶åˆ°åŽç«‹å³æ‰§è¡Œä¸Šè¿°ç”Ÿæˆé€»è¾‘ã€‚**`;

// ====== AI SERVICE CLASS ======

class AIService {
    constructor() {
        this.provider = localStorage.getItem('ai_provider') || 'groq';
        this.apiKey = localStorage.getItem('ai_api_key') || '';
        this.model = localStorage.getItem('ai_model') || '';
        this.conversationHistory = [];
        this.currentPhase = 'architect'; // 'architect' | 'engine'
        this.abortController = null;
    }

    // ---- Config ----

    getConfig() {
        return {
            provider: this.provider,
            apiKey: this.apiKey,
            model: this.model,
        };
    }

    setConfig({ provider, apiKey, model }) {
        this.provider = provider;
        this.apiKey = apiKey;
        this.model = model || this.getDefaultModel(provider);
        localStorage.setItem('ai_provider', this.provider);
        localStorage.setItem('ai_api_key', this.apiKey);
        localStorage.setItem('ai_model', this.model);
    }

    getDefaultModel(provider) {
        switch (provider) {
            case 'groq': return 'llama-3.3-70b-versatile';
            case 'gemini': return 'gemini-2.0-flash';
            case 'glm': return 'glm-4-flash';
            default: return '';
        }
    }

    isConfigured() {
        return !!this.apiKey;
    }

    // ---- List available models ----

    async listModels(provider, apiKey) {
        try {
            if (provider === 'groq') {
                return await this._listGroqModels(apiKey);
            } else if (provider === 'gemini') {
                return await this._listGeminiModels(apiKey);
            } else if (provider === 'glm') {
                return await this._listGlmModels(apiKey);
            }
            return [];
        } catch (err) {
            throw err;
        }
    }

    async _listGroqModels(apiKey) {
        try {
            const response = await fetch('https://api.groq.com/openai/v1/models', {
                headers: { 'Authorization': `Bearer ${apiKey}` },
            });
            if (!response.ok) {
                console.warn(`Groq models fetch failed (${response.status}), using fallback list.`);
                throw new Error(`API Key æ— æ•ˆæˆ–è¯·æ±‚å¤±è´¥ (${response.status})`);
            }
            const data = await response.json();
            const models = (data.data || [])
                .map(m => ({ id: m.id, name: m.id }))
                .sort((a, b) => a.name.localeCompare(b.name));
            return models;
        } catch (err) {
            // Fallback to manual list if API fails (likely due to CORS or key)
            return [
                { id: 'llama-3.3-70b-versatile', name: 'llama-3.3-70b-versatile' },
                { id: 'llama-3.1-70b-versatile', name: 'llama-3.1-70b-versatile' },
                { id: 'llama-3.1-8b-instant', name: 'llama-3.1-8b-instant' },
                { id: 'mixtral-8x7b-32768', name: 'mixtral-8x7b-32768' },
                { id: 'gemma2-9b-it', name: 'gemma2-9b-it' },
                { id: 'llama3-70b-8192', name: 'llama3-70b-8192' },
                { id: 'llama3-8b-8192', name: 'llama3-8b-8192' },
            ];
        }
    }

    async _listGeminiModels(apiKey) {
        const response = await fetch(
            `https://generativelanguage.googleapis.com/v1beta/models?key=${apiKey}`
        );
        if (!response.ok) {
            throw new Error(`API Key æ— æ•ˆæˆ–è¯·æ±‚å¤±è´¥ (${response.status})`);
        }
        const data = await response.json();
        const models = (data.models || [])
            .filter(m => m.supportedGenerationMethods?.includes('generateContent'))
            .map(m => ({
                id: m.name.replace('models/', ''),
                name: m.displayName || m.name.replace('models/', ''),
            }))
            .sort((a, b) => a.name.localeCompare(b.name));
        return models;
    }

    async _listGlmModels(apiKey) {
        const response = await fetch('https://open.bigmodel.cn/api/paas/v4/models', {
            headers: { 'Authorization': `Bearer ${apiKey}` },
        });
        if (!response.ok) {
            throw new Error(`API Key æ— æ•ˆæˆ–è¯·æ±‚å¤±è´¥ (${response.status})`);
        }
        const data = await response.json();
        const models = (data.data || [])
            .filter(m => m.id && !m.id.includes('embedding'))
            .map(m => ({ id: m.id, name: m.id }))
            .sort((a, b) => a.name.localeCompare(b.name));
        return models;
    }

    // ---- Phase management ----

    setPhase(phase) {
        this.currentPhase = phase;
    }

    getSystemPrompt() {
        return this.currentPhase === 'engine'
            ? SYSTEM_PROMPT_ENGINE
            : SYSTEM_PROMPT_ARCHITECT;
    }

    // ---- Conversation ----

    resetConversation() {
        this.conversationHistory = [];
        this.currentPhase = 'architect';
    }

    switchToEngine(reportText) {
        // Switch phase and inject the report as context
        this.currentPhase = 'engine';
        this.conversationHistory = [
            { role: 'user', content: reportText }
        ];
    }

    // ---- State persistence ----

    exportState() {
        return {
            conversationHistory: [...this.conversationHistory],
            currentPhase: this.currentPhase,
        };
    }

    importState(state) {
        if (state) {
            this.conversationHistory = state.conversationHistory || [];
            this.currentPhase = state.currentPhase || 'architect';
        }
    }

    // ---- API Calls ----

    async sendMessage(userMessage, onChunk) {
        if (!this.isConfigured()) {
            throw new Error('è¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½® API Key');
        }

        // Add user message to history
        this.conversationHistory.push({
            role: 'user',
            content: userMessage,
        });

        // Build messages array with system prompt
        const messages = [
            { role: 'system', content: this.getSystemPrompt() },
            ...this.conversationHistory,
        ];

        this.abortController = new AbortController();

        let fullResponse = '';

        try {
            if (this.provider === 'groq') {
                fullResponse = await this._callGroq(messages, onChunk);
            } else if (this.provider === 'gemini') {
                fullResponse = await this._callGemini(messages, onChunk);
            } else if (this.provider === 'glm') {
                fullResponse = await this._callGlm(messages, onChunk);
            } else {
                throw new Error(`Unknown provider: ${this.provider}`);
            }

            // Add assistant response to history
            this.conversationHistory.push({
                role: 'assistant',
                content: fullResponse,
            });

            return fullResponse;
        } catch (err) {
            // Remove the user message if call failed
            this.conversationHistory.pop();
            throw err;
        }
    }

    abort() {
        if (this.abortController) {
            this.abortController.abort();
        }
    }

    // ---- Groq API (OpenAI compatible) ----

    async _callGroq(messages, onChunk) {
        const model = this.model || 'llama-3.3-70b-versatile';
        const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${this.apiKey}`,
            },
            body: JSON.stringify({
                model,
                messages,
                stream: true,
            }),
            signal: this.abortController.signal,
        });

        if (!response.ok) {
            const errText = await response.text();
            throw new Error(`Groq API é”™è¯¯ (${response.status}): ${errText}`);
        }

        return this._readSSEStream(response.body, onChunk);
    }

    // ---- GLM API (OpenAI compatible) ----

    async _callGlm(messages, onChunk) {
        const model = this.model || 'glm-4-flash';
        const response = await fetch('https://open.bigmodel.cn/api/paas/v4/chat/completions', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${this.apiKey}`,
            },
            body: JSON.stringify({
                model,
                messages,
                stream: true,
            }),
            signal: this.abortController.signal,
        });

        if (!response.ok) {
            const errText = await response.text();
            throw new Error(`GLM API é”™è¯¯ (${response.status}): ${errText}`);
        }

        return this._readSSEStream(response.body, onChunk);
    }

    // ---- Gemini API ----

    async _callGemini(messages, onChunk) {
        const model = this.model || 'gemini-2.0-flash';
        const url = `https://generativelanguage.googleapis.com/v1beta/models/${model}:streamGenerateContent?alt=sse&key=${this.apiKey}`;

        // Convert OpenAI format to Gemini format
        const systemInstruction = messages.find(m => m.role === 'system');
        const contents = messages
            .filter(m => m.role !== 'system')
            .map(m => ({
                role: m.role === 'assistant' ? 'model' : 'user',
                parts: [{ text: m.content }],
            }));

        const body = {
            contents,
            generationConfig: {
                temperature: 0.7,
            },
        };

        if (systemInstruction) {
            body.systemInstruction = {
                parts: [{ text: systemInstruction.content }],
            };
        }

        const response = await fetch(url, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(body),
            signal: this.abortController.signal,
        });

        if (!response.ok) {
            const errText = await response.text();
            throw new Error(`Gemini API é”™è¯¯ (${response.status}): ${errText}`);
        }

        return this._readGeminiSSEStream(response.body, onChunk);
    }

    // ---- Stream Readers ----

    async _readSSEStream(body, onChunk) {
        const reader = body.getReader();
        const decoder = new TextDecoder();
        let fullText = '';
        let buffer = '';

        while (true) {
            const { done, value } = await reader.read();
            if (done) break;

            buffer += decoder.decode(value, { stream: true });
            const lines = buffer.split('\n');
            buffer = lines.pop() || '';

            for (const line of lines) {
                const trimmed = line.trim();
                if (!trimmed || !trimmed.startsWith('data: ')) continue;
                const data = trimmed.slice(6);
                if (data === '[DONE]') continue;

                try {
                    const json = JSON.parse(data);
                    const delta = json.choices?.[0]?.delta?.content;
                    if (delta) {
                        fullText += delta;
                        if (onChunk) onChunk(delta, fullText);
                    }
                } catch (e) {
                    // skip parse errors
                }
            }
        }

        return fullText;
    }

    async _readGeminiSSEStream(body, onChunk) {
        const reader = body.getReader();
        const decoder = new TextDecoder();
        let fullText = '';
        let buffer = '';

        while (true) {
            const { done, value } = await reader.read();
            if (done) break;

            buffer += decoder.decode(value, { stream: true });
            const lines = buffer.split('\n');
            buffer = lines.pop() || '';

            for (const line of lines) {
                const trimmed = line.trim();
                if (!trimmed || !trimmed.startsWith('data: ')) continue;
                const data = trimmed.slice(6);

                try {
                    const json = JSON.parse(data);
                    const text = json.candidates?.[0]?.content?.parts?.[0]?.text;
                    if (text) {
                        fullText += text;
                        if (onChunk) onChunk(text, fullText);
                    }
                } catch (e) {
                    // skip parse errors
                }
            }
        }

        return fullText;
    }

    // ---- Auto-start architect ----

    async startArchitect(onChunk) {
        this.resetConversation();
        // Send empty init message to trigger the first question
        const initMessages = [
            { role: 'system', content: this.getSystemPrompt() },
            { role: 'user', content: 'ä½ å¥½ï¼Œæˆ‘å‡†å¤‡å¥½äº†ï¼Œè¯·å¼€å§‹ã€‚' },
        ];

        this.abortController = new AbortController();
        let fullResponse = '';

        try {
            if (this.provider === 'groq') {
                fullResponse = await this._callGroqDirect(initMessages, onChunk);
            } else if (this.provider === 'gemini') {
                fullResponse = await this._callGeminiDirect(initMessages, onChunk);
            } else if (this.provider === 'glm') {
                fullResponse = await this._callGlmDirect(initMessages, onChunk);
            } else {
                throw new Error(`Unknown provider: ${this.provider}`);
            }

            this.conversationHistory.push(
                { role: 'user', content: 'ä½ å¥½ï¼Œæˆ‘å‡†å¤‡å¥½äº†ï¼Œè¯·å¼€å§‹ã€‚' },
                { role: 'assistant', content: fullResponse }
            );

            return fullResponse;
        } catch (err) {
            throw err;
        }
    }

    async _callGroqDirect(messages, onChunk) {
        const model = this.model || 'llama-3.3-70b-versatile';
        const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${this.apiKey}`,
            },
            body: JSON.stringify({ model, messages, stream: true }),
            signal: this.abortController.signal,
        });

        if (!response.ok) {
            const errText = await response.text();
            throw new Error(`Groq API é”™è¯¯ (${response.status}): ${errText}`);
        }

        return this._readSSEStream(response.body, onChunk);
    }

    async _callGlmDirect(messages, onChunk) {
        const model = this.model || 'glm-4-flash';
        const response = await fetch('https://open.bigmodel.cn/api/paas/v4/chat/completions', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${this.apiKey}`,
            },
            body: JSON.stringify({ model, messages, stream: true }),
            signal: this.abortController.signal,
        });

        if (!response.ok) {
            const errText = await response.text();
            throw new Error(`GLM API é”™è¯¯ (${response.status}): ${errText}`);
        }

        return this._readSSEStream(response.body, onChunk);
    }

    async _callGeminiDirect(messages, onChunk) {
        const model = this.model || 'gemini-2.0-flash';
        const url = `https://generativelanguage.googleapis.com/v1beta/models/${model}:streamGenerateContent?alt=sse&key=${this.apiKey}`;

        const systemInstruction = messages.find(m => m.role === 'system');
        const contents = messages
            .filter(m => m.role !== 'system')
            .map(m => ({
                role: m.role === 'assistant' ? 'model' : 'user',
                parts: [{ text: m.content }],
            }));

        const body = { contents, generationConfig: { temperature: 0.7 } };
        if (systemInstruction) {
            body.systemInstruction = { parts: [{ text: systemInstruction.content }] };
        }

        const response = await fetch(url, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(body),
            signal: this.abortController.signal,
        });

        if (!response.ok) {
            const errText = await response.text();
            throw new Error(`Gemini API é”™è¯¯ (${response.status}): ${errText}`);
        }

        return this._readGeminiSSEStream(response.body, onChunk);
    }
}

// Export as global singleton
window.aiService = new AIService();
